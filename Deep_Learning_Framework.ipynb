{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1.What is TensorFlow 2.0, and how is it different from TensorFlow 1.x2?\n",
        "TensorFlow 2.0 is a major upgrade of Google’s open-source deep learning framework TensorFlow. Released in September 2019, it was designed to make TensorFlow simpler, more user-friendly, and more Pythonic while retaining flexibility for research and production.\n",
        "Its main goal was to fix many pain points in TensorFlow 1.x, such as its complexity, steep learning curve, and verbose code.\n"
      ],
      "metadata": {
        "id": "2GkwCgyD4ShP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.How do you install TensorFlow 2.0?\n",
        "Installation steps for Tensorflow 2.0:\n",
        "1.TensorFlow 2.0 works with Python 3.5, 3.6, or 3.7.\n",
        "python --version\n",
        "pip --version\n",
        "2.Create a Virtual Environment.\n",
        "This keeps your TensorFlow setup clean and avoids conflicts.\n",
        "# Create a new environment (example: tf2_env)\n",
        "python -m venv tf2_env\n",
        "# Activate it\n",
        "# On Windows:\n",
        "tf2_env\\Scripts\\activate\n",
        "# On Mac/Linux:\n",
        "source tf2_env/bin/activate\n",
        "3.Install TensorFlow 2.0\n",
        "install TensorFlow 2.0 specifically using pip:\n",
        "4.Verify Installation\n",
        "Run Python and check the version:\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NwETGCTH4gCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.What is the primary function of the tf.function in TensorFlow 2.0?\n",
        "The tf.function decorator in TensorFlow 2.0 is used to convert a regular Python function into a TensorFlow computation graph.\n",
        "This brings together the ease of eager execution (default in TF2) and the performance of graph execution (from TF1).\n",
        "Benefits:\n",
        "Faster execution (graph optimization).\n",
        "Compatible with serving/export.\n",
        "Simplifies moving between eager mode (easy debugging) and graph mode (high performance)."
      ],
      "metadata": {
        "id": "GTe3jdlP7c0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.What is the purpose of the Model class in TensorFlow 2.0?\n",
        "In TensorFlow 2.0, the Model class (specifically tf.keras.Model) is the core class for defining and training neural network models.\n",
        "It provides a high-level API for:\n",
        "Building models (sequential or complex architectures).\n",
        "Training models with .fit().\n",
        "Evaluating models with .evaluate().\n",
        "Making predictions with .predict().\n",
        "Exporting and saving models for deployment.\n",
        "The Model class is the foundation of deep learning in TensorFlow 2.0.\n",
        "Purpose: define, train, evaluate, predict, and save models.\n",
        "It makes TensorFlow more user-friendly and consistent by unifying everything under tf.keras.Model."
      ],
      "metadata": {
        "id": "a4oYHQM--OVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.How do you create a neural network using TensorFlow 2.0?\n",
        "In TensorFlow 2.0, you can create a neural network in three main ways:\n",
        "1.Sequential API (simplest, layer-by-layer)\n",
        "2.Functional API (more flexible, for complex models)\n",
        "3.Subclassing tf.keras.Model (full customization)"
      ],
      "metadata": {
        "id": "tWXnv9HP-xsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6.What is the importance of Tensor Space in TensorFlow?\n",
        "Tensor Space in TensorFlow 2.0 = the mathematical environment where all tensors (data, parameters, gradients) exist and interact.\n",
        "Importance:\n",
        "Provides a unified structure for handling all forms of data.\n",
        "Defines how tensors transform between layers.\n",
        "Enables efficient computation and optimization in deep learning."
      ],
      "metadata": {
        "id": "iNSgiOrE_ZFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.How can TensorBoard be integrated with TensorFlow 2.0?\n",
        "TensorBoard is the official visualization toolkit for TensorFlow, and in TensorFlow 2.0 it integrates very smoothly.\n",
        "TensorBoard in TF 2.0 integrates via the TensorBoard callback in model.fit().\n",
        "Just:Create log directory\n",
        "Add TensorBoard callback\n",
        "Run tensorboard --logdir logs/fit\n"
      ],
      "metadata": {
        "id": "2FyzU5u6_vwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8.What is the purpose of TensorFlow Playground?\n",
        "TensorFlow Playground is an interactive web-based tool (created by Google) that lets you experiment with neural networks in the browser.\n",
        "TensorFlow Playground = A browser-based tool to play with neural networks visually.\n",
        "Purpose: To learn, experiment, and build intuition about how neural networks learn, without needing to code in TensorFlow/Python.\n"
      ],
      "metadata": {
        "id": "ioE3VpvLBBQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9.What is Netron, and how is it useful for deep learning models?\n",
        "Netron is an open-source model visualization tool for deep learning and machine learning models.\n",
        "It is a cross-platform app (desktop + web) that allows you to view, inspect, and explore neural network architectures.\n",
        "Supports many frameworks: TensorFlow, PyTorch, Keras, ONNX, Caffe, MXNet, CoreML, TFLite, and more.\n",
        "Netron is extremely useful for debugging, understanding, and sharing models:\n",
        "1.Model Architecture Visualization\n",
        "Displays layers, connections, input/output shapes, and parameter counts.\n",
        "Makes it easy to understand complex architectures.\n",
        "2.Supports Multiple File Formats\n",
        "Works with .pb (TensorFlow), .h5 (Keras), .onnx, .pt (PyTorch), .tflite, .mlmodel, etc.\n",
        "3.Inspect Layer Details\n",
        "we can click on layers to see parameters (e.g., kernel size, stride, activation).\n",
        "Also shows number of trainable weights.\n",
        "4.Cross-Framework Compatibility\n",
        "If you export models (e.g., PyTorch → ONNX → TensorRT), Netron helps check if the conversion worked properly.\n",
        "5.Debugging & Validation\n",
        "Helps detect mismatched input/output shapes.\n",
        "Ensures the exported model is correct before deployment.\n",
        "5.Educational Tool\n",
        "Great for students or researchers to learn how architectures (ResNet, LSTM, GANs, etc.) are structured."
      ],
      "metadata": {
        "id": "S39hgR_GB-hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10.What is the difference between TensorFlow and PyTorch?\n",
        "Both are popular open-source deep learning frameworks, but they differ in design philosophy, execution style, and ecosystem.\n",
        "PyTorch is used if want flexibility, easier debugging, and research prototyping.\n",
        "TensorFlow is used if we care about deployment, scalability, and production pipelines."
      ],
      "metadata": {
        "id": "f1KPuBCmC6Oi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11.How do you install PyTorch?\n",
        "To install PyTorch,we need:\n",
        "Which package manager→pip or conda\n",
        "Which build→CPU-only or GPU(CUDA/ROCm)\n",
        "Installation with pip:\n",
        "CPU only:\n",
        "pip install torch torchvision torchaudio\n",
        "GPU:\n",
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "Installation with conda:\n",
        "CPU only:\n",
        "conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
        "GPU:\n",
        "conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
        "Verify Installation:\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BBrEZmSqDtdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12.What is the basic structure of a PyTorch neural network?\n",
        "A neural network is usually built by subclassing the torch.nn.Module class. The basic structure has two main parts:\n",
        "1.__init__ method – where you define the layers.\n",
        "2.forward method – where you describe how data flows through those layers.\n",
        "1.Model as a Class\n",
        "In PyTorch, every neural network is defined as a class that inherits from torch.nn.Module.\n",
        "This class acts as a blueprint for your model.\n",
        "2.Initialization (__init__)\n",
        "This is where you define the layers of your network (e.g., linear layers, convolutional layers, LSTMs).\n",
        "Each layer specifies how inputs will be transformed.\n",
        "3.Forward Pass (forward)\n",
        "This defines how the data flows through the layers.\n",
        "apply operations such as linear transformations, activations (ReLU, Sigmoid, etc.), pooling, dropout, etc.\n",
        "The output of this method is the model’s prediction.\n",
        "4.Parameters\n",
        "Each layer contains learnable parameters (weights and biases).\n",
        "PyTorch automatically keeps track of these parameters for optimization.\n",
        "5.Loss Function\n",
        "A mathematical function that measures the difference between predictions and the actual target (e.g., CrossEntropyLoss, MSELoss).\n",
        "6.Optimizer\n",
        "An algorithm (e.g., SGD, Adam) that updates the parameters using gradients computed from backpropagation.\n",
        "7.Training Loop\n",
        "Repeated steps where:\n",
        "1.Input data is passed through the network (forward pass).\n",
        "2.Loss is computed.\n",
        "3.Gradients are calculated(backward pass).\n",
        "4.Optimizer updates the parameters.\n"
      ],
      "metadata": {
        "id": "UxgWHrQiJryU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13.What is the significance of tensors in PyTorch?\n",
        "Tensors are the core building blocks in PyTorch.\n",
        "Significance of Tensors in PyTorch:\n",
        "1.Fundamental Data Structure\n",
        "A tensor is a multi-dimensional array (generalization of vectors and matrices).\n",
        "All inputs, outputs, and model parameters in PyTorch are represented as tensors.\n",
        "2.Efficient Computation\n",
        "Tensors can be processed efficiently on CPUs, GPUs, and TPUs.\n",
        "PyTorch automatically uses optimized libraries (like BLAS, cuDNN, MKL) for fast tensor operations.\n",
        "3.Automatic Differentiation (Autograd)\n",
        "PyTorch tensors can be tracked with autograd to automatically compute gradients.\n",
        "This makes backpropagation in neural networks possible without manually computing derivatives.\n",
        "4.Flexibility in Dimensions\n",
        "Tensors can represent:Scalars (0D tensors),Vectors (1D tensors),Matrices (2D tensors)\n",
        "Higher-dimensional data (3D for images, 4D/5D for video or batches of data, etc.)\n",
        "5.Bridge Between Data and Models\n",
        "Input data (e.g., images, text, audio) is converted into tensors.\n",
        "Model weights and biases are stored as tensors.\n",
        "Training updates these tensors iteratively.\n",
        "6.Seamless GPU Acceleration\n",
        "A tensor can be easily moved between CPU and GPU with:\n",
        "tensor.to(\"cuda\")   # move to GPU  \n",
        "tensor.to(\"cpu\")    # move back to CPU\n",
        "This allows large-scale deep learning computations to run much faster."
      ],
      "metadata": {
        "id": "LNpIs0t_OCNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#14.What is the difference between torch.Tensor and torch.cuda.Tensor in PyTorch?\n",
        "Difference Between torch.Tensor and torch.cuda.Tensor\n",
        "1.Device Location\n",
        "torch.Tensor → by default lives on the CPU.\n",
        "torch.cuda.Tensor → lives on the GPU (uses NVIDIA CUDA for acceleration).\n",
        "2.Computation Speed\n",
        "CPU tensors (torch.Tensor) → better for small computations, debugging, or systems without a GPU.\n",
        "GPU tensors (torch.cuda.Tensor) → allow parallel computation and are much faster for large-scale deep learning tasks.\n",
        "3.Creation\n",
        "By default, calling torch.tensor([...]) creates a CPU tensor.\n",
        "To create a GPU tensor, we can either:\n",
        "x = torch.tensor([1, 2, 3], device=\"cuda\")  # directly on GPU\n",
        "or move it:\n",
        "x = torch.tensor([1, 2, 3])  # CPU tensor\n",
        "x = x.to(\"cuda\")             # move to GPU\n",
        "4.Compatibility\n",
        "we cannot directly perform operations between CPU and GPU tensors.\n",
        "cpu_tensor + gpu_tensor  # ❌ RuntimeError\n",
        "One must be moved to the other’s device first.\n",
        "5.Autograd Support\n",
        "Both support automatic differentiation (requires_grad=True).\n",
        "On GPU, gradient computations are accelerated using CUDA libraries."
      ],
      "metadata": {
        "id": "KCQTstawPXyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#15.What is the purpose of the torch.optim module in PyTorch?\n",
        "The torch.optim module provides optimization algorithms used for training neural networks.\n",
        "Purpose of torch.optim\n",
        "1.Parameter Updates (Learning Process)\n",
        "Training a neural network means adjusting weights and biases to minimize the loss.\n",
        "torch.optim contains implementations of standard optimization algorithms (SGD, Adam, RMSprop, etc.) that update model parameters automatically using gradients.\n",
        "2.Works with Autograd\n",
        "Gradients are computed using autograd (loss.backward()).\n",
        "torch.optim takes these gradients and applies an update rule to adjust the parameters.\n",
        "3.Encapsulation of Optimization Logic\n",
        "Instead of writing weight-update formulas manually, you define an optimizer that:\n",
        "Knows which parameters to update (model.parameters()).\n",
        "Knows how to update them (algorithm + learning rate).\n",
        "4.Support for Hyperparameters\n",
        "Each optimizer accepts hyperparameters like:Learning rate (lr),Momentum (for SGD),Weight decay (for regularization)\n",
        "These can be easily tuned during training."
      ],
      "metadata": {
        "id": "v-AUdi-YR24f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#16.What are some common activation functions used in neural networks?\n",
        "Activation functions are crucial in neural networks because they introduce non-linearity,allowing networks to learn complex patterns.\n",
        "Some Common Activation Functions in Neural Networks:\n",
        "Sigmoid & Tanh → older, risk vanishing gradients.\n",
        "ReLU & Leaky ReLU → most common in hidden layers.\n",
        "Softmax → classification outputs.\n",
        "Swish/GELU → modern, often better in deep architectures.\n"
      ],
      "metadata": {
        "id": "eSgoQxLhTxip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#17.What is the difference between torch.nn.Module and torch.nn.Sequential in PyTorch?\n",
        "**torch.nn.Module:**\n",
        "General base class for all neural networks in PyTorch.\n",
        "subclass nn.Module to define custom architectures.\n",
        "Requires us to explicitly write:\n",
        "__init__ → define layers\n",
        "forward → define how data flows through those layers\n",
        "Allows flexibility:\n",
        "Multiple inputs/outputs\n",
        "Conditional logic\n",
        "Residual connections (e.g., ResNet)\n",
        "Loops or complex architectures\n",
        "Best for complex/custom models.\n",
        "**torch.nn.Sequential:**\n",
        "A container module that stacks layers in a linear sequence.\n",
        "don’t define a forward method explicitly → PyTorch applies each layer in order.\n",
        "Simple and concise for straightforward models (layer → activation → layer → …).\n",
        "Best for simple feedforward models where data flows in one direction without branching."
      ],
      "metadata": {
        "id": "1yDnoUPNU7tx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#18.How can you monitor training progress in TensorFlow 2.0?\n",
        "In TensorFlow 2.x, monitoring training progress is an essential part of deep learning workflow. TensorFlow provides multiple built-in tools for tracking metrics, losses, and visualizations.\n",
        "Ways to Monitor Training Progress in TensorFlow 2.x\n",
        "1.Using verbose in model.fit()\n",
        "The simplest way:\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "verbose=1 → shows a progress bar with loss and metrics for each epoch.\n",
        "verbose=2 → shows per-epoch results (no bar).\n",
        "2.Using the History object\n",
        "model.fit() returns a History object containing losses and metrics.\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "Lets plot training vs validation curves over epochs.\n",
        "3.Callbacks\n",
        "TensorFlow provides callbacks to track and log training progress. Some common ones:\n",
        "tf.keras.callbacks.EarlyStopping → stop training when validation loss stops improving.\n",
        "tf.keras.callbacks.ModelCheckpoint → save model checkpoints during training.\n",
        "tf.keras.callbacks.ReduceLROnPlateau → reduce learning rate when training stagnates.\n",
        "4.TensorBoard (most powerful)\n",
        "TensorFlow integrates tightly with TensorBoard, a visualization toolkit.\n",
        "5.Custom Callback for Printing/Logging\n",
        "define your own callback to monitor anything (e.g., gradient norms, intermediate metrics)."
      ],
      "metadata": {
        "id": "AuX29yVdV110"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#19.How does the Keras API fit into TensorFlow 2.02?\n",
        "In TensorFlow 1.x, Keras was a separate library (keras), though we could also use it through tf.keras.\n",
        "In TensorFlow 2.x, tf.keras is tightly integrated and is the recommended way to build models.\n",
        "from tensorflow import kerasKeras provides a simplified, user-friendly API to define and train models without worrying about the low-level details of TensorFlow operations.\n",
        "It supports both the Sequential API (stacking layers linearly) and the Functional/Model Subclassing API (for more complex architectures).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MlAyDaq3bJCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#20.What is an example of a deep learning project that can be implemented using TensorFlow 2.0?\n",
        "1.Sentiment Analysis on movie reviews (NLP).\n",
        "2.Speech Recognition with RNNs or Transformers.\n",
        "3.Chatbot using sequence-to-sequence models.\n",
        "4.Object Detection (using pre-trained models like Faster R-CNN or YOLO).\n",
        "5.Medical Imaging (classifying X-ray scans)."
      ],
      "metadata": {
        "id": "dmHJ5dL6DAde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#21.What is the main advantage of using pre-trained models in TensorFlow and PyTorch?\n",
        "The main advantage of using pre-trained models in both TensorFlow and PyTorch is:\n",
        "Transfer Learning:\n",
        "Instead of training a deep learning model from scratch (which requires huge datasets and a lot of compute power), you can start with a pre-trained model that has already learned rich feature representations from a large dataset (like ImageNet for vision, or BERT for NLP).\n",
        "Then you can fine-tune it on your own (usually smaller) dataset for your specific task."
      ],
      "metadata": {
        "id": "-d6HiUBDEqoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.How do you install and verify that TensorFlow 2.0 was installed successfully?\n",
        "Installation:\n",
        "pip install tensorflow==2.0.0\n",
        "conda create -n tf2 python=3.7\n",
        "conda activate tf2\n",
        "pip install tensorflow==2.0.0\n",
        "Verify installation:\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cBZYHCcaFFU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.How can you define a simple function in TensorFlow 2.0 to perform addition?\n",
        "import tensorflow as tf\n",
        "# Define the function\n",
        "@tf.function\n",
        "def add_numbers(x, y):\n",
        "    return tf.add(x, y)\n",
        "# Test it\n",
        "a = tf.constant(5)\n",
        "b = tf.constant(7)\n",
        "result = add_numbers(a, b)\n",
        "print(\"Result:\", result.numpy())\n"
      ],
      "metadata": {
        "id": "KFGeRXDpFrSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.How can you create a simple neural network in TensorFlow 2.0 with one hidden layer?\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "# Define the model\n",
        "model = models.Sequential([\n",
        "    layers.Dense(128, activation='relu', input_shape=(784,)),  # hidden layer\n",
        "    layers.Dense(10, activation='softmax')                     # output layer\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# Load sample dataset (MNIST digits)\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "# Preprocess: flatten 28x28 images → 784 vector, normalize to [0,1]\n",
        "x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
        "# Evaluate\n",
        "loss, acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(\"Test accuracy:\", acc)\n"
      ],
      "metadata": {
        "id": "M5AiAyDqGr-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.How can you visualize the training progress using TensorFlow and Matplotlib?\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "# Load dataset (MNIST for example)\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "x_train = x_train.reshape(-1, 28*28)\n",
        "x_test = x_test.reshape(-1, 28*28)\n",
        "# Define a simple model\n",
        "model = models.Sequential([\n",
        "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# Train the model and store history\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_test, y_test))\n",
        "# Plot training & validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Model Accuracy')\n",
        "plt.show()\n",
        "# Plot training & validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Model Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XCNoJiQNG-Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5.How do you install PyTorch and verify the PyTorch installation?\n",
        "pip install torch torchvision torchaudio\n",
        "import torch\n",
        "# Check version\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "# Check if GPU is available\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "# If CUDA is available, print GPU name\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n"
      ],
      "metadata": {
        "id": "PfFbgljOHkV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6.How do you create a simple neural network in PyTorch?\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "# Transform: convert to tensor & normalize (0–1)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Download MNIST\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)  # input → hidden\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)     # hidden → output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)   # flatten image\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)  # input → hidden\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)     # hidden → output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)   # flatten image\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "model = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "for epoch in range(5):  # 5 epochs\n",
        "    for images, labels in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/5], Loss: {loss.item():.4f}\")\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"Test Accuracy:\", 100 * correct / total, \"%\")\n"
      ],
      "metadata": {
        "id": "4ELLBVgEH7ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7.How do you define a loss function and optimizer in PyTorch?\n",
        "import torch.nn as nn\n",
        "# For classification (e.g., MNIST digits)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "import torch.optim as optim\n",
        "# Assume 'model' is your neural network\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "for images, labels in train_loader:\n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()   # clear old gradients\n",
        "    loss.backward()         # compute new gradients\n",
        "    optimizer.step()        # update weights\n",
        "\n"
      ],
      "metadata": {
        "id": "QFNNgu0aIkyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8.How do you implement a custom loss function in PyTorch?\n",
        "import torch\n",
        "def custom_mae_loss(y_pred, y_true):\n",
        "    return torch.mean(torch.abs(y_pred - y_true))\n",
        "# Example usage\n",
        "y_true = torch.tensor([1.0, 2.0, 3.0])\n",
        "y_pred = torch.tensor([1.5, 2.5, 2.0])\n",
        "loss = custom_mae_loss(y_pred, y_true)\n",
        "print(\"Custom MAE Loss:\", loss.item())\n",
        "criterion = CustomHuberLoss(delta=1.0)\n",
        "for data, target in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(data)\n",
        "    loss = criterion(outputs, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "W-x2A5AiI4eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9.How do you save and load a TensorFlow model?\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Build a simple model\n",
        "model = models.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(784,)),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Save the whole model\n",
        "model.save(\"my_model\")\n",
        "\n",
        "# Load it back\n",
        "loaded_model = tf.keras.models.load_model(\"my_model\")\n",
        "\n",
        "# Verify\n",
        "print(\"Loaded model summary:\")\n",
        "loaded_model.summary()\n"
      ],
      "metadata": {
        "id": "-HvaZWkLJQWH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}